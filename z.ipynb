{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "# Sybil Attack Detection with Multiple Classifiers<br>\n",
    "This code evaluates the performance of different classifiers on the Sybil Attack dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [module imports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "## Data Extraction<br>\n",
    "Load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [data loading]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_full_training_set = \"../Sybil Attack Detection/New folder/KDDTrain+.txt\"\n",
    "file_path_test = \"../Sybil Attack Detection/New folder/KDDTest+.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path_full_training_set)\n",
    "test_df = pd.read_csv(file_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "## Data Preprocessing<br>\n",
    "Add column names and create binary attack flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [data preprocessing]<br>\n",
    "Add column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"duration\",\n",
    "    \"protocol_type\",\n",
    "    \"service\",\n",
    "    \"flag\",\n",
    "    \"src_bytes\",\n",
    "    \"dst_bytes\",\n",
    "    \"land\",\n",
    "    \"wrong_fragment\",\n",
    "    \"urgent\",\n",
    "    \"hot\",\n",
    "    \"num_failed_logins\",\n",
    "    \"logged_in\",\n",
    "    \"num_compromised\",\n",
    "    \"root_shell\",\n",
    "    \"su_attempted\",\n",
    "    \"num_root\",\n",
    "    \"num_file_creations\",\n",
    "    \"num_shells\",\n",
    "    \"num_access_files\",\n",
    "    \"num_outbound_cmds\",\n",
    "    \"is_host_login\",\n",
    "    \"is_guest_login\",\n",
    "    \"count\",\n",
    "    \"srv_count\",\n",
    "    \"serror_rate\",\n",
    "    \"srv_serror_rate\",\n",
    "    \"rerror_rate\",\n",
    "    \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\n",
    "    \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\",\n",
    "    \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\",\n",
    "    \"attack\",\n",
    "    \"level\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = columns\n",
    "test_df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary attack flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"attack_flag\"] = df[\"attack\"].map(lambda a: 0 if a == \"normal\" else 1)\n",
    "test_df[\"attack_flag\"] = test_df[\"attack\"].map(lambda a: 0 if a == \"normal\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "## Feature Engineering<br>\n",
    "Encode categorical features and prepare the dataset for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [feature engineering]<br>\n",
    "One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_encode = [\"protocol_type\", \"service\", \"flag\"]\n",
    "encoded = pd.get_dummies(df[features_to_encode])\n",
    "test_encoded_base = pd.get_dummies(test_df[features_to_encode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle column differences between train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.arange(len(test_df.index))\n",
    "column_diffs = list(set(encoded.columns.values) - set(test_encoded_base.columns.values))\n",
    "diff_df = pd.DataFrame(0, index=test_index, columns=column_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder columns to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = encoded.columns.to_list()\n",
    "test_encoded_temp = test_encoded_base.join(diff_df)\n",
    "test_final = test_encoded_temp[column_order].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"duration\", \"src_bytes\", \"dst_bytes\"]\n",
    "to_fit = encoded.join(df[numeric_features])\n",
    "test_set = test_final.join(test_df[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_y = df[\"attack_flag\"]\n",
    "test_binary_y = test_df[\"attack_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_X, binary_val_X, binary_train_y, binary_val_y = train_test_split(\n",
    "    to_fit, binary_y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "## Model Training and Evaluation<br>\n",
    "Train and evaluate multiple classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [model evaluation]<br>\n",
    "Define the models to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(binary_val_X)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43maccuracy\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(binary_train_X, binary_train_y)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(binary_val_X)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
